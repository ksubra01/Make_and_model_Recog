{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCyGij6oMoHF",
        "outputId": "f04bc98b-36b1-4c50-ea51-a5aa3de9bfa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAA1gqk9Qmau"
      },
      "source": [
        "Read the images from the directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "limGNJoVIZvy"
      },
      "outputs": [],
      "source": [
        "#Function for getting the image and corresponding label from directory\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def label_image(path_to_data):\n",
        "\n",
        "\tlist_label = os.listdir(path_to_data) # Get the list of files in the required directory\n",
        "\n",
        "\tlabelmapping = {}\n",
        "\tfor ind, value in enumerate(list_label): # the names of the vehicles are used as keys and are assigned a number,their index position, as values.\n",
        "\t\tlabelmapping[value] = ind\n",
        "\n",
        "\tx = []\n",
        "\tlabel = []\n",
        "\n",
        "\tfor lab in list_label:\n",
        "\t\tfor img in glob.glob(path_to_data+\"/\" + lab + '/*'):\n",
        "\t\t\timage = cv2.imread(img)\n",
        "\t\t\tLabel = labelmapping[lab]\n",
        "\n",
        "\t\t\tx.append(image)\n",
        "\t\t\tlabel.append(Label)\n",
        "\n",
        "\treturn x, label, labelmapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDBZ3D3gQrcV"
      },
      "source": [
        "Creation of Feature vector (SIFT + BOW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "waOFzo5qOb_b"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from sklearn.cluster import KMeans\n",
        "import pickle\n",
        "from scipy.spatial.distance import cdist\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def SIFT_Feature_extraction(list_image):\n",
        "\n",
        "    descriptors = []\n",
        "    sift = cv2.xfeatures2d.SIFT_create()                      # Create an instantiation of SIFT object.\n",
        "    for image in list_image:\n",
        "        _, descriptor = sift.detectAndCompute(image, None)    # Use the instantiation to obtain the keypoints and descriptors.\n",
        "        descriptors.append(descriptor)\n",
        "\n",
        "    return descriptors\n",
        "\n",
        "def Clustering(descriptors, vocab_size):                      # This function creates the groups the descriptors into k(= vocabulary size) clusters \n",
        "    bow_dict = []\n",
        "\n",
        "    kmeans = KMeans(n_clusters = vocab_size)\n",
        "    kmeans.fit(descriptors)\n",
        "\n",
        "    bow_dict = kmeans.cluster_centers_\n",
        "\n",
        "    if not os.path.isfile('BoW_dict.pkl'):\n",
        "        pickle.dump(bow_dict, open('BoW_dict.pkl', 'wb'))     # The pickle function is used to store the k clusters(the words learnt) into a .pkl file. This is the visual dictionary\n",
        "\n",
        "    return bow_dict\n",
        "\n",
        "def feature_vector_creation(image_descriptors, BoW, num_cluster):\n",
        "\n",
        "    X_features = []\n",
        "\n",
        "    for i in range(len(image_descriptors)):\n",
        "        features = np.array([0] * num_cluster)\n",
        "\n",
        "        if image_descriptors[i] is not None:\n",
        "            distance = cdist(image_descriptors[i], BoW)\n",
        "\n",
        "            argmin = np.argmin(distance, axis = 1)\n",
        "\n",
        "            for j in argmin:\n",
        "                features[j] += 1\n",
        "        X_features.append(features)\n",
        "\n",
        "    return X_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_30wBMvOmPh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import sklearn\n",
        "import pickle\n",
        "import argparse\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "training_data, data_label, label2id = label_image('/content/drive/MyDrive/data/Training_data/') # Use the required path here\n",
        "\n",
        "cv2.imwrite('test.jpg', training_data[0])\n",
        "print(\"the id mapping of this class for this image is:\",data_label[5])\n",
        "print(\" The make and model is:\")\n",
        "for u in label2id.keys():\n",
        "  if label2id[u] == data_label[5]:\n",
        "    print(u)\n",
        "\n",
        "\n",
        "image_desctiptors = SIFT_Feature_extraction(training_data)\n",
        "\n",
        "all_descriptors = []\n",
        "# Cleans the data. There are occaionally, some unwanted None values in the descriptors. The following piece of code removes that.\n",
        "for descriptor in image_desctiptors:\n",
        "    if descriptor is not None:\n",
        "        for des in descriptor:\n",
        "            all_descriptors.append(des)\n",
        "\n",
        "vocab_size = 20\n",
        "BoW = Clustering(all_descriptors, vocab_size)\n",
        "\n",
        "X_features = feature_vector_creation(image_desctiptors, BoW, vocab_size)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_features, data_label, test_size = 0.3)\n",
        "\n",
        "\n",
        "parameters = {'kernel':('linear','poly', 'rbf'), 'C': [1,2,4,6,8,10,12]} # Which function to fit and what value of regularization constant to fit?\n",
        "param = {'linear','poly', 'rbf','sigmoid'}\n",
        "\n",
        "ker = parameters['kernel']\n",
        "Cc = parameters['C']\n",
        "\n",
        "Test_acc_linear = []\n",
        "Train_acc_linear = []\n",
        "\n",
        "Test_acc_rbf = []\n",
        "Train_acc_rbf = []\n",
        "\n",
        "Test_acc_poly = []\n",
        "Train_acc_poly = []\n",
        "k = 0\n",
        "for j in ker:\n",
        "\n",
        "  for i in range(len(Cc)):\n",
        "\n",
        "\n",
        "  # grid_model = GridSearchCV(model_svm,parameters)\n",
        "  # grid_model.fit(X_train, Y_train)\n",
        "    model_svm = sklearn.svm.SVC(C = Cc[i], kernel = j,random_state = 1)\n",
        "    model_svm.fit(X_train, Y_train)\n",
        "    filename = 'Support_Vector_Machine.sav'\n",
        "    pickle.dump(model_svm, open(filename, 'wb'))\n",
        "    print(\"score on training set params: \", model_svm.score(X_train, Y_train))\n",
        "    print(\"score on testing set params: \", model_svm.score(X_test, Y_test))\n",
        "\n",
        "    if j == 'linear':\n",
        "      Train_acc_linear.append(100*float(model_svm.score(X_train, Y_train)))\n",
        "\n",
        "      Test_acc_linear.append(100*float(model_svm.score(X_test, Y_test)))\n",
        "    elif j == 'rbf':\n",
        "       Train_acc_rbf.append(100*float(model_svm.score(X_train, Y_train)))\n",
        "\n",
        "       Test_acc_rbf.append(100*float(model_svm.score(X_test, Y_test))) \n",
        "    else:\n",
        "       Train_acc_poly.append(100*float(model_svm.score(X_train, Y_train)))\n",
        "\n",
        "       Test_acc_poly.append(100*float(model_svm.score(X_test, Y_test)))    \n",
        "\n",
        "\n",
        "  # print(\"best score: \", grid_model.best_score_)\n",
        "  # print(\"best_params: \", grid_model.best_params_)\n",
        "# print(data_train[0])\n",
        "\n",
        "# print(label2id)\n",
        "\n",
        "# cv2.imwrite(data_label[0],data_train[0])\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(Cc,Test_acc_linear,label='Testing Accuracy')\n",
        "plt.plot(Cc,Train_acc_linear,label='Training Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Regularization constant\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Train and Test Accuracy on Cross Validation using linear SVM kernel\")\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(Cc,Test_acc_rbf,label='Testing Accuracy')\n",
        "plt.plot(Cc,Train_acc_rbf,label='Training Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Regularization constant\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Train and Test Accuracy on Cross Validation using rbf SVM kernel\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(Cc,Test_acc_poly,label='Testing Accuracy')\n",
        "plt.plot(Cc,Train_acc_poly,label='Training Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Regularization constant\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Train and Test Accuracy on Cross Validation using poly SVM kernel\")\n",
        "\n",
        "\n",
        "\n",
        "grid_model = GridSearchCV(model_svm,parameters)\n",
        "grid_model.fit(X_train, Y_train)\n",
        "grid_model.fit(X_test,Y_test) \n",
        "print(\"best choice of parameters using grid search CV: \", grid_model.best_params_)\n",
        "print(\"best accuracy using grid search CV: \", grid_model.best_score_)\n",
        "\n",
        "##################### Testing Data #############################\n",
        "\n",
        "testing_data, testing_label, label2id = label_image('/content/drive/MyDrive/data/Test_data/')\n",
        "image_desctiptors1 = SIFT_Feature_extraction(testing_data)\n",
        "\n",
        "all_descriptors1 = []\n",
        "for descriptor in image_desctiptors1:\n",
        "    if descriptor is not None:\n",
        "        for des in descriptor:\n",
        "            all_descriptors1.append(des)\n",
        "\n",
        "vocab_size = 20\n",
        "BoW1 = Clustering(all_descriptors1, vocab_size)\n",
        "\n",
        "X_features1 = feature_vector_creation(image_desctiptors1, BoW1, vocab_size)\n",
        "grid_model.fit(X_features1,testing_label)\n",
        "\n",
        "\n",
        "print(\"--------------------------------------\")\n",
        "print(\"Best Accuracy for test data\")\n",
        "print(\"best accuracy on test set using grid search: \", grid_model.best_score_)\n",
        "print(\"best parameters on test set using grid search: \", grid_model.best_params_)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}